{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import classification_report,accuracy_score,balanced_accuracy_score"
   ]
  },
  {
   "source": [
    "# 1. TEXT CLASSIFICATION: IMDB data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "data    = pd.read_csv(\"imdb.csv\",index_col=0)\n",
    "X_train,X_test,y_train,y_test = ?"
   ]
  },
  {
   "source": [
    "#### Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Family 0.14511424404472534\nSci-Fi 0.33908604764219735\nThriller 0.1276130286825474\nRomance 0.3881866796305299\n"
     ]
    }
   ],
   "source": [
    "categories      = [\"Family\",\"Sci-Fi\",\"Thriller\", \"Romance\"]\n",
    "n_classes       = len(categories)\n",
    "train_texts     = list(X_train[\"Plot\"].values)\n",
    "test_texts      = list(X_test[\"Plot\"].values)\n",
    "train_labels_genre=list(y_train.values)\n",
    "test_labels_genre=list(y_test.values)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(categories)\n",
    "train_labels = le.transform(train_labels_genre)\n",
    "test_labels = le.transform(test_labels_genre)\n",
    "print(\"Class balances:\")\n",
    "for i,c in enumerate(categories):\n",
    "    print(c,np.mean(train_labels==i))"
   ]
  },
  {
   "source": [
    "# GloVe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Cleaning the text for GloVe:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation)).lower()"
   ]
  },
  {
   "source": [
    "#### Converting the words to vectors and take the average for each movie plot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# PCA of movie plots"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Classify genres based on the first n principal components of GloVe vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# FastText Classification using 3-gram embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "# Adapted from: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "#########################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import TextClassificationDataset\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "#####################################################################################################################\n",
    "# Auxilary functions\n",
    "#####################################################################################################################\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "\n",
    "def token_iterator(texts, ngrams):\n",
    "    for text in texts:\n",
    "        tokens = tokenizer(text)\n",
    "        yield ngrams_iterator(tokens, ngrams)\n",
    "\n",
    "\n",
    "def construct_vocab(texts, ngrams):\n",
    "    vocab = build_vocab_from_iterator(token_iterator(texts, ngrams))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def text_to_tensor(text, vocab, ngrams):\n",
    "    tokens = ngrams_iterator(tokenizer(text), ngrams=ngrams)\n",
    "    token_ids = list(filter(lambda x: x is not Vocab.UNK, [vocab[token] for token in tokens]))\n",
    "    tokens = torch.tensor(token_ids)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def make_torchdataset(vocab, texts, labels, ngrams):\n",
    "    tokens = [text_to_tensor(text, vocab, ngrams) for text in tqdm(texts)]\n",
    "    pairs = list(zip(labels, tokens))\n",
    "    return TextClassificationDataset(vocab, pairs, set(labels))\n",
    "\n",
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label\n",
    "\n",
    "#####################################################################################################################\n",
    "# Model\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "#####################################################################################################################\n",
    "# FastText\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "class FastText(object):\n",
    "\n",
    "    def __init__(self, texts, labels, embed_dim, ngrams=3, num_epochs=5, seed=0):\n",
    "\n",
    "        # set seed\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ngrams = ngrams\n",
    "\n",
    "        # construct vocab\n",
    "        print('Constructing vocabulary...')\n",
    "        self.vocab = construct_vocab(texts, ngrams)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # prepare dataset\n",
    "        print('Preparing dataset...')\n",
    "        self.train_dataset = make_torchdataset(self.vocab, texts, labels, ngrams)\n",
    "        self.num_classes = len(self.train_dataset.get_labels())\n",
    "\n",
    "        # prepare device ref and model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = TextClassificationModel(self.vocab_size, self.embed_dim, self.num_classes).to(self.device)\n",
    "\n",
    "        # loss function & optimization\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=4.0)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1, gamma=0.9)\n",
    "        self.batch_size = 16\n",
    "\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.ngrams = ngrams\n",
    "\n",
    "\n",
    "\n",
    "        if num_epochs > 0:\n",
    "            print('Training model...')\n",
    "            self.train(self.train_dataset, num_epochs)\n",
    "\n",
    "\n",
    "    def train_step(self, sub_train_):\n",
    "\n",
    "        # Train the model\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        data = DataLoader(sub_train_, batch_size=self.batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "        for i, (text, offsets, cls) in enumerate(data):\n",
    "            self.optimizer.zero_grad()\n",
    "            text, offsets, cls = text.to(self.device), offsets.to(self.device), cls.to(self.device)\n",
    "            output = self.model(text, offsets)\n",
    "            loss = self.criterion(output, cls)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "        # Adjust the learning rate\n",
    "        self.scheduler.step()\n",
    "\n",
    "        return train_loss / len(sub_train_),  train_acc / len(sub_train_)\n",
    "\n",
    "    def compute_loss(self, data_):\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        data = DataLoader(data_, batch_size=self.batch_size, collate_fn=generate_batch)\n",
    "        for text, offsets, cls in data:\n",
    "            text, offsets, cls = text.to(self.device), offsets.to(self.device), cls.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                output = self.model(text, offsets)\n",
    "                loss = self.criterion(output, cls)\n",
    "                loss += loss.item()\n",
    "                acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "        return loss / len(data_), acc / len(data_)\n",
    "\n",
    "    def train(self, train_dataset, n_epochs=5):\n",
    "\n",
    "        min_valid_loss = float('inf')\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc  = self.train_step(train_dataset)\n",
    "\n",
    "            secs = int(time.time() - start_time)\n",
    "            mins = secs / 60\n",
    "            secs = secs % 60\n",
    "\n",
    "            print('Epoch: %d' % (epoch + 1), \" | time in %d minutes, %d seconds\" % (mins, secs))\n",
    "            print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "        print('')\n",
    "\n",
    "    def predict(self, text_, return_prob=False):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text = text_to_tensor(text_, self.vocab, self.ngrams)\n",
    "            output = self.model(text, torch.tensor([0]))\n",
    "\n",
    "            if return_prob:\n",
    "                return F.softmax(output, 1).detach().numpy()\n",
    "            else:\n",
    "                return output.argmax(1).item()\n",
    "\n",
    "    def get_text_embedding(self, text_):\n",
    "        with torch.no_grad():\n",
    "            text = text_to_tensor(text_, self.vocab, self.ngrams)\n",
    "            return self.model.embedding(text, offsets=torch.LongTensor([0])).detach().numpy()\n",
    "\n",
    "    def word_in_vocab(self, word):\n",
    "        return word in self.vocab.stoi\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1021lines [00:00, 10202.66lines/s]Constructing vocabulary...\n",
      "4114lines [00:00, 11792.53lines/s]\n",
      " 18%|█▊        | 739/4114 [00:00<00:00, 7383.58it/s]Preparing dataset...\n",
      "100%|██████████| 4114/4114 [00:00<00:00, 6135.94it/s]\n",
      "Training model...\n",
      "Epoch: 1  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0815(train)\t|\tAcc: 42.7%(train)\n",
      "Epoch: 2  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0600(train)\t|\tAcc: 59.9%(train)\n",
      "Epoch: 3  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0371(train)\t|\tAcc: 80.1%(train)\n",
      "Epoch: 4  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0207(train)\t|\tAcc: 92.4%(train)\n",
      "Epoch: 5  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0114(train)\t|\tAcc: 98.2%(train)\n",
      "Epoch: 6  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0072(train)\t|\tAcc: 99.7%(train)\n",
      "Epoch: 7  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0050(train)\t|\tAcc: 99.8%(train)\n",
      "Epoch: 8  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0038(train)\t|\tAcc: 99.9%(train)\n",
      "Epoch: 9  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0031(train)\t|\tAcc: 99.9%(train)\n",
      "Epoch: 10  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 99.9%(train)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "model = FastText(train_texts,train_labels,embed_dim=100,num_epochs=10)"
   ]
  },
  {
   "source": [
    "# Classification on test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# PCA on FastText embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Write your own plot and see what genre GloVe+PCA+classifier and FastText guess it is"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Optional: Do classification/regression on another variable in the dataset, such as predicting movie ratings based on year and plot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2. SENTIMENT ANALYSIS: The Donald"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.loadtxt(\"Donald.txt\",dtype='str')\n",
    "text_cleaned = []\n",
    "for word in text:\n",
    "    \n",
    "    # to lower\n",
    "    doc_cleaned = word.lower()\n",
    "    \n",
    "    #doc_cleaned = ' '.join([word for word in doc_cleaned.split() if word not in stop_words])    \n",
    "    doc_cleaned = doc_cleaned.replace('.', '')\n",
    "    doc_cleaned = doc_cleaned.replace(',', '')\n",
    "    doc_cleaned = doc_cleaned.replace('?', '')\n",
    "    doc_cleaned = doc_cleaned.replace('!', '')\n",
    "    text_cleaned.append(doc_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Chief' 'Justice' 'Roberts,' ... 'God' 'bless' 'America.']\n['chief', 'justice', 'roberts', 'president', 'carter', 'president', 'clinton', 'president', 'bush', 'president', 'obama', 'fellow', 'americans', 'and', 'people', 'of', 'the', 'world', 'thank', 'you', 'we', 'the', 'citizens', 'of', 'america', 'are', 'now', 'joined', 'in', 'a', 'great', 'national', 'effort', 'to', 'rebuild', 'our', 'country', 'and', 'restore', 'its', 'promise', 'for', 'all', 'of', 'our', 'people', 'together', 'we', 'will', 'determine', 'the', 'course', 'of', 'america', 'and', 'the', 'world', 'for', 'many', 'many', 'years', 'to', 'come', 'we', 'will', 'face', 'challenges', 'we', 'will', 'confront', 'hardships', 'but', 'we', 'will', 'get', 'the', 'job', 'done', 'every', 'four', 'years', 'we', 'gather', 'on', 'these', 'steps', 'to', 'carry', 'out', 'the', 'orderly', 'and', 'peaceful', 'transfer', 'of', 'power', 'and', 'we', 'are', 'grateful', 'to', 'president', 'obama', 'and', 'first', 'lady', 'michelle', 'obama', 'for', 'their', 'gracious', 'aid', 'throughout', 'this', 'transition', 'they', 'have', 'been', 'magnificent', 'thank', \"youtoday's\", 'ceremony', 'however', 'has', 'very', 'special', 'meaning', 'because', 'today', 'we', 'are', 'not', 'merely', 'transferring', 'power', 'from', 'one', 'administration', 'to', 'another', 'or', 'from', 'one', 'party', 'to', 'another', 'but', 'we', 'are', 'transferring', 'power', 'from', 'washington', 'dc', 'and', 'giving', 'it', 'back', 'to', 'you', 'the', 'peoplefor', 'too', 'long', 'a', 'small', 'group', 'in', 'our', \"nation's\", 'capital', 'has', 'reaped', 'the', 'rewards', 'of', 'government', 'while', 'the', 'people', 'have', 'borne', 'the', 'cost', 'washington', 'flourished', 'but', 'the', 'people', 'did', 'not', 'share', 'in', 'its', 'wealth', 'politicians', 'prospered', 'but', 'the', 'jobs', 'left', 'and', 'the', 'factories', 'closed', 'the', 'establishment', 'protected', 'itself', 'but', 'not', 'the', 'citizens', 'of', 'our', 'country', 'their', 'victories', 'have', 'not', 'been', 'your', 'victories', 'their', 'triumphs', 'have', 'not', 'been', 'your', 'triumphs', 'and', 'while', 'they', 'celebrated', 'in', 'our', \"nation's\", 'capital', 'there', 'was', 'little', 'to', 'celebrate', 'for', 'struggling', 'families', 'all', 'across', 'our', 'land', 'that', 'all', 'changes', 'starting', 'right', 'here', 'and', 'right', 'now', 'because', 'this', 'moment', 'is', 'your', 'moment', '---', 'it', 'belongs', 'to', 'you', 'it', 'belongs', 'to', 'everyone', 'gathered', 'here', 'today', 'and', 'everyone', 'watching', 'all', 'across', 'america', 'this', 'is', 'your', 'day', 'this', 'is', 'your', 'celebration', 'and', 'this', 'the', 'united', 'states', 'of', 'america', 'is', 'your', 'countrywhat', 'truly', 'matters', 'is', 'not', 'which', 'party', 'controls', 'our', 'government', 'but', 'whether', 'our', 'government', 'is', 'controlled', 'by', 'the', 'people', 'january', '20th', '2017', 'will', 'be', 'remembered', 'as', 'the', 'day', 'the', 'people', 'became', 'the', 'rulers', 'of', 'this', 'nation', 'again', 'the', 'forgotten', 'men', 'and', 'women', 'of', 'our', 'country', 'will', 'be', 'forgotten', 'no', 'longer', 'everyone', 'is', 'listening', 'to', 'you', 'now', 'you', 'came', 'by', 'the', 'tens', 'of', 'millions', 'to', 'become', 'part', 'of', 'a', 'historic', 'movement', 'the', 'likes', 'of', 'which', 'the', 'world', 'has', 'never', 'seen', 'before', 'at', 'the', 'center', 'of', 'this', 'movement', 'is', 'a', 'crucial', 'conviction', 'that', 'a', 'nation', 'exists', 'to', 'serve', 'its', 'citizens', 'americans', 'want', 'great', 'schools', 'for', 'their', 'children', 'safe', 'neighborhoods', 'for', 'their', 'families', 'and', 'good', 'jobs', 'for', 'themselves', 'these', 'are', 'just', 'and', 'reasonable', 'demands', 'of', 'righteous', 'people', 'and', 'a', 'righteous', 'public', 'but', 'for', 'too', 'many', 'of', 'our', 'citizens', 'a', 'different', 'reality', 'exists', 'mothers', 'and', 'children', 'trapped', 'in', 'poverty', 'in', 'our', 'inner', 'cities', 'rusted', 'out', 'factories', 'scattered', 'like', 'tombstones', 'across', 'the', 'across', 'the', 'landscape', 'of', 'our', 'nation', 'an', 'education', 'system', 'flush', 'with', 'cash', 'but', 'which', 'leaves', 'our', 'young', 'and', 'beautiful', 'students', 'deprived', 'of', 'all', 'knowledge', 'and', 'the', 'crime', 'and', 'the', 'gangs', 'and', 'the', 'drugs', 'that', 'have', 'stolen', 'too', 'many', 'lives', 'and', 'robbed', 'our', 'country', 'of', 'so', 'much', 'unrealized', 'potential', 'this', 'american', 'carnage', 'stops', 'right', 'here', 'and', 'stops', 'right', 'now', 'we', 'are', 'one', 'nation', 'and', 'their', 'pain', 'is', 'our', 'pain', 'their', 'dreams', 'are', 'our', 'dreams', 'and', 'their', 'success', 'will', 'be', 'our', 'success', 'we', 'share', 'one', 'heart', 'one', 'home', 'and', 'one', 'glorious', 'destiny', 'the', 'oath', 'of', 'office', 'i', 'take', 'today', 'is', 'an', 'oath', 'of', 'allegiance', 'to', 'all', 'americans', 'for', 'many', 'decades', \"we've\", 'enriched', 'foreign', 'industry', 'at', 'the', 'expense', 'of', 'american', 'industry', 'subsidized', 'the', 'armies', 'of', 'other', 'countries', 'while', 'allowing', 'for', 'the', 'very', 'sad', 'depletion', 'of', 'our', 'military', \"we've\", 'defended', 'other', \"nation's\", 'borders', 'while', 'refusing', 'to', 'defend', 'our', 'own', 'and', 'spent', 'trillions', 'and', 'trillions', 'of', 'dollars', 'overseas', 'while', \"america's\", 'infrastructure', 'has', 'fallen', 'into', 'disrepair', 'and', 'decay', \"we've\", 'made', 'other', 'countries', 'rich', 'while', 'the', 'wealth', 'strength', 'and', 'confidence', 'of', 'our', 'country', 'has', 'dissipated', 'over', 'the', 'horizon', 'one', 'by', 'one', 'the', 'factories', 'shuddered', 'and', 'left', 'our', 'shores', 'with', 'not', 'even', 'a', 'thought', 'about', 'the', 'millions', 'and', 'millions', 'of', 'american', 'workers', 'that', 'were', 'left', 'behind', 'the', 'wealth', 'of', 'our', 'middle', 'class', 'has', 'been', 'ripped', 'from', 'their', 'homes', 'and', 'then', 'redistributed', 'all', 'across', 'the', 'world', 'but', 'that', 'is', 'the', 'past', 'and', 'now', 'we', 'are', 'looking', 'only', 'to', 'the', 'future', 'we', 'assembled', 'here', 'today', 'our', 'issuing', 'a', 'new', 'decree', 'to', 'be', 'heard', 'in', 'every', 'city', 'in', 'every', 'foreign', 'capital', 'and', 'in', 'every', 'hall', 'of', 'power', 'from', 'this', 'day', 'forward:', 'a', 'new', 'vision', 'will', 'govern', 'our', 'land', 'from', 'this', 'day', 'forward', \"it's\", 'going', 'to', 'be', 'only', 'america', 'first', 'america', 'first', 'every', 'decision', 'on', 'trade', 'on', 'taxes', 'on', 'immigration', 'on', 'foreign', 'affairs', 'will', 'be', 'made', 'to', 'benefit', 'american', 'workers', 'and', 'american', 'families', 'we', 'must', 'protect', 'our', 'borders', 'from', 'the', 'ravages', 'of', 'other', 'countries', 'making', 'our', 'products', 'stealing', 'our', 'companies', 'and', 'destroying', 'our', 'jobs', 'protection', 'will', 'lead', 'to', 'great', 'prosperity', 'and', 'strength', 'i', 'will', 'fight', 'for', 'you', 'with', 'every', 'breath', 'in', 'my', 'body', 'and', 'i', 'will', 'never', 'ever', 'let', 'you', 'down', 'america', 'will', 'start', 'winning', 'again', 'winning', 'like', 'never', 'before', 'we', 'will', 'bring', 'back', 'our', 'jobs', 'we', 'will', 'bring', 'back', 'our', 'borders', 'we', 'will', 'bring', 'back', 'our', 'wealth', 'and', 'we', 'will', 'bring', 'back', 'our', 'dreams', 'we', 'will', 'build', 'new', 'roads', 'and', 'highways', 'and', 'bridges', 'and', 'airports', 'and', 'tunnels', 'and', 'railways', 'all', 'across', 'our', 'wonderful', 'nation', 'we', 'will', 'get', 'our', 'people', 'off', 'of', 'welfare', 'and', 'back', 'to', 'work', 'rebuilding', 'our', 'country', 'with', 'american', 'hands', 'and', 'american', 'labor', 'we', 'will', 'follow', 'two', 'simple', 'rules:', 'buy', 'american', 'and', 'hire', 'american', 'we', 'will', 'seek', 'friendship', 'and', 'goodwill', 'with', 'the', 'nations', 'of', 'the', 'world', 'but', 'we', 'do', 'so', 'with', 'the', 'understanding', 'that', 'it', 'is', 'the', 'right', 'of', 'all', 'nations', 'to', 'put', 'their', 'own', 'interests', 'first', 'we', 'do', 'not', 'seek', 'to', 'impose', 'our', 'way', 'of', 'life', 'on', 'anyone', 'but', 'rather', 'to', 'let', 'it', 'shine', 'as', 'an', 'example', 'we', 'will', 'shine', 'for', 'everyone', 'to', 'follow', 'we', 'will', 'reinforce', 'old', 'alliances', 'and', 'form', 'new', 'ones', 'and', 'you', 'unite', 'the', 'civilized', 'world', 'against', 'radical', 'islamic', 'terrorism', 'which', 'we', 'will', 'eradicate', 'completely', 'from', 'the', 'face', 'of', 'the', 'earthat', 'the', 'bedrock', 'of', 'our', 'politics', 'will', 'be', 'a', 'total', 'allegiance', 'to', 'the', 'united', 'states', 'of', 'america', 'and', 'through', 'our', 'loyalty', 'to', 'our', 'country', 'we', 'will', 'rediscover', 'our', 'loyalty', 'to', 'each', 'other', 'when', 'you', 'open', 'your', 'heart', 'to', 'patriotism', 'there', 'is', 'no', 'room', 'for', 'prejudice', 'the', 'bible', 'tells', 'us', 'how', 'good', 'and', 'pleasant', 'it', 'is', 'when', \"god's\", 'people', 'live', 'together', 'in', 'unity', 'we', 'must', 'speak', 'our', 'minds', 'openly', 'debate', 'our', 'disagreements', 'but', 'always', 'pursue', 'solidarity', 'when', 'america', 'is', 'united', 'america', 'is', 'totally', 'unstoppable', 'there', 'should', 'be', 'no', 'fear', 'we', 'are', 'protected', 'and', 'we', 'will', 'always', 'be', 'protected', 'we', 'will', 'be', 'protected', 'by', 'the', 'great', 'men', 'and', 'women', 'of', 'our', 'military', 'and', 'law', 'enforcement', 'and', 'most', 'importantly', 'we', 'will', 'be', 'protected', 'by', 'godfinally', 'we', 'must', 'think', 'big', 'and', 'dream', 'even', 'bigger', 'in', 'america', 'we', 'understand', 'that', 'a', 'nation', 'is', 'only', 'living', 'as', 'long', 'as', 'it', 'is', 'striving', 'we', 'will', 'no', 'longer', 'accept', 'politicians', 'who', 'are', 'all', 'talk', 'and', 'no', 'action', 'constantly', 'complaining', 'but', 'never', 'doing', 'anything', 'about', 'it', 'the', 'time', 'for', 'empty', 'talk', 'is', 'over', 'now', 'arrives', 'the', 'hour', 'of', 'action', 'do', 'not', 'allow', 'anyone', 'to', 'tell', 'you', 'that', 'it', 'cannot', 'be', 'done', 'no', 'challenge', 'can', 'match', 'the', 'heart', 'and', 'fight', 'and', 'spirit', 'of', 'america', 'we', 'will', 'not', 'fail', 'our', 'country', 'will', 'thrive', 'and', 'prosper', 'againwe', 'stand', 'at', 'the', 'birth', 'of', 'a', 'new', 'millennium', 'ready', 'to', 'unlock', 'the', 'mysteries', 'of', 'space', 'to', 'free', 'the', 'earth', 'from', 'the', 'miseries', 'of', 'disease', 'and', 'to', 'harness', 'the', 'industries', 'and', 'technologies', 'of', 'tomorrow', 'a', 'new', 'national', 'pride', 'will', 'stir', 'our', 'souls', 'lift', 'our', 'sights', 'and', 'heal', 'our', 'divisions', \"it's\", 'time', 'to', 'remember', 'that', 'old', 'wisdom', 'our', 'soldiers', 'will', 'never', 'forget', 'that', 'whether', 'we', 'are', 'black', 'or', 'brown', 'or', 'white', 'we', 'all', 'bleed', 'the', 'same', 'red', 'blood', 'of', 'patriots', 'we', 'all', 'enjoy', 'the', 'same', 'glorious', 'freedoms', 'and', 'we', 'all', 'salute', 'the', 'same', 'great', 'american', 'flag', 'and', 'whether', 'a', 'child', 'is', 'born', 'in', 'the', 'urban', 'sprawl', 'of', 'detroit', 'or', 'the', 'windswept', 'plains', 'of', 'nebraska', 'they', 'look', 'up', 'at', 'the', 'at', 'the', 'same', 'night', 'sky', 'they', 'fill', 'their', 'heart', 'with', 'the', 'same', 'dreams', 'and', 'they', 'are', 'infused', 'with', 'the', 'breath', 'of', 'life', 'by', 'the', 'same', 'almighty', 'creatorso', 'to', 'all', 'americans', 'in', 'every', 'city', 'near', 'and', 'far', 'small', 'and', 'large', 'from', 'mountain', 'to', 'mountain', 'from', 'ocean', 'to', 'ocean', 'hear', 'these', 'words', 'you', 'will', 'never', 'be', 'ignored', 'again', 'your', 'voice', 'your', 'hopes', 'and', 'your', 'dreams', 'will', 'define', 'our', 'american', 'destiny', 'and', 'your', 'courage', 'and', 'goodness', 'and', 'love', 'will', 'forever', 'guide', 'us', 'along', 'the', 'way', 'together', 'we', 'will', 'make', 'america', 'strong', 'again', 'we', 'will', 'make', 'america', 'wealthy', 'again', 'we', 'will', 'make', 'america', 'proud', 'again', 'we', 'will', 'make', 'america', 'safe', 'again', 'and', 'yes', 'together', 'we', 'will', 'make', 'we', 'will', 'make', 'america', 'great', 'again', 'thank', 'you', 'god', 'bless', 'you', 'and', 'god', 'bless', 'america', 'thank', 'you', 'god', 'bless', 'america']\n"
     ]
    }
   ],
   "source": [
    "print(text_cleaned)"
   ]
  },
  {
   "source": [
    "# Calculate window-wise sentiment and have the window size and stride as variables that can easily be changed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Plot the sentiment over time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Low pass filter using a smoothing filter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Optional: multiply the sentiment-lexicon with a positive constant of your own choosing and comment on what effects it has on the sentiment analysis plot. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3610jvsc74a57bd00fdbd2cf40c6c7674f69ebc7ae9cac97a2b9701cdf9e2a2397be25d69aca8b5f",
   "display_name": "Python 3.6.10 64-bit ('quantum': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}